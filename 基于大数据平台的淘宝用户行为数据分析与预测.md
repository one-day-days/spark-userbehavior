# 基于大数据平台的淘宝用户行为数据分析与预测



## 一、设计介绍

​     	电子商务行业是一个充满竞争的市场。当下，各大电商企业都在寻找新的方法来提高销售量和客户回头率，而预测用户购买行为就成为了电商企业必须掌握的技能之一。
​		随着时间的推移，传统的人工智能方法已经不能满足业务需求。因此，数据挖掘、机器学习等技术越来越被电商企业采用[2]，以实现更精准的用户行为预测。通过这些技术，企业可以利用大量的数据并提取出有价值的信息，实现对用户需求的更好了解，从而更好地满足消费者的需求并提高销售额。
​		目前，电商企业在数据挖掘方面已经取得了一些突破，例如运用机器学习算法进行用户画像、预测用户需求、商品推荐等。同时，企业也在逐渐将新技术引入到各个环节中，包括物流、支付、客服等。这使得电商企业在与传统零售业的竞争中保持了明显优势。
总之，电子商务行业的快速发展和不断变化，使得企业需要采用更加精确和先进的技术手段来预测用户购买行为，满足消费者需求，提高市场竞争力。
​		本篇论文的研究目标是通过Spark数据挖掘预测出用户在2014年12月18日会购买的商品。利用淘宝用户行为数据（userBehavior）和商品数据集(item)，采用LR、RF、GBDT等三种算法进行分析，通过比较不同算法的表现，选出能够最好预测用户购买行为的模型。
具体研究内容包括以下方面：
数据集分析：对淘宝用户行为数据集（userBehavior）和商品数据集(item)进行了解，了解数据规模、特征分布等信息。
模型选择和训练：对比LR、RF、GBDT三种模型，选择最适合的模型，并使用Spark进行模型训练。
模型评估和优化：通过验证集验证模型的准确性，进行模型参数的调整和优化，提高模型的预测精度。
结果分析：分析各种算法的表现，选出最佳算法，得出预测结果。
本论文的研究成果可以为电子商务企业提供更加准确的用户购买行为预测，提高企业的竞争力和营销效果。



## 二、开发技术

### 1.Spark SQL

​		Spark SQL是Apache Spark的一个模块，提供了用于结构化数据处理的高级数据处理接口。它允许Spark执行SQL查询、将关系数据加载为DataFrame和DataSet对象，并与Spark程序的其他部分集成。Spark SQL使用Catalyst作为优化器，可优化SQL查询计划的性能。

### 2.Spark MLib

​		Spark MLlib是一个开源的机器学习框架，是Apache Spark的组成部分。它提供了一系列的机器学习算法和工具，包括分类、回归、聚类、协同过滤等。Spark MLlib支持分布式计算，并且可以在分布式集群上进行大规模的机器学习任务。它的设计目标是提供易于使用、高效的机器学习算法和工具，以及与Spark生态系统的良好集成。

### 3.Pandas

​		Pandas是一个Python语言的开源数据分析库，广泛应用于数据科学、机器学习、数据挖掘等领域。Pandas提供了一种名为DataFrame的数据结构，类似于电子表格或SQL表，可以方便地处理和分析结构化数据。Pandas可以处理各种类型的数据，包括时间序列数据和非结构化数据。



## 三、算法

### 1 逻辑回归（LR）

逻辑回归（Logistic Regression）是一种常见的分类算法。它是一种基于概率的统计分类模型，常用于二分类问题，例如预测用户是否会购买某个产品。逻辑回归通过将自变量与概率转换为一个0或1的输出，来进行分类预测。在用户行为数据挖掘中，可以使用逻辑回归来分析用户的行为特征，并预测用户是否会执行某种操作。

### 2 随机森林（RF）

随机森林（Random Forest）是一种集成学习算法，可以用于分类和回归问题。它由多个决策树组成[7]，每个决策树都是通过在数据集中随机抽样形成的。随机森林算法可以解决决策树的过拟合问题，并能够有效处理高维数据。在用户行为数据挖掘中，可以使用随机森林算法来识别用户的行为模式，并预测用户是否会采取某种行为。

### 3 梯度提升树（GBDT）  

梯度提升树(Gradient Boosting Tree)也是一种集成学习算法，可用于分类和回归问题。与随机森林不同，GBDT由多个弱分类器组成，每个弱分类器都是通过逐步学习来提高性能。GBDT算法适用于非线性可分的数据集，可以有效处理高维数据和非线性数据。在用户行为数据挖掘中，可以使用GBDT算法来提取用户行为数据的关键特征，并预测用户是否会采取某种行为。



## 四、数据分析

### 1.数据简介

#### 4.1.1 用户行为数据

​		第一部分为用户在电子商务平台移动端对商品全集的行为数据（D），文件名为tianchi_mobile_recommend_train_user.zip(实际为partA和partB两部分)，详细字段如表4.1 所示。数据集D在zip压缩格式下大小为8G，解压后为Text格式约40G。



| 字段名称      | 字段说明           | 提取                           |
| ------------- | ------------------ | ------------------------------ |
| user_id       | 用户标识           | 抽样&字段脱敏                  |
| item_id       | 商品标识           | 字段脱敏                       |
| behavior_type | 用户对商品行为类型 | 包括浏览、收藏、加购物车、购买 |
| user_geohash  | 用户位置空间标识   | 由经纬度通过保密的算法生成     |
| item_category | 商品分类标识       | 字段脱敏                       |
| time          | 行为时间           | 精确到小时                     |

 

#### 4.1.2 商品数据集

​		第二部分为商品数据集（P），大小为41MB。

| 字段          | 字段说明                     | 提取说明                    |
| ------------- | ---------------------------- | --------------------------- |
| item_id       | 商品标识                     | 抽样&字段脱敏               |
| item_geohash  | 商品位置的空间标识，可以为空 | 由经纬度·通过保密的算法生成 |
| item_category | 商品分类标识                 | 字段脱敏                    |



### 2.数据分析

#### 4.2.1 流量分析

本课题的数据分析先从宏观的流量分析入手，分析数据中有哪些规律。流量分析主要关注PV与UV。访问量(PV)：全名为Page View, 基于用户每次对淘宝页面的刷新次数，用户每刷新一次页面或者打开新的页面就记录就算一次访问；独立访问量(UV)：全名为Unique Visitor，一个用户若多次访问淘宝只记录一次。

① 基于天级别访问流量分析

统计每天的用户访问量，主要代码如下：

```scala
val df = spark.read.orc(inpath)

df.createOrReplaceTempView("user")

val sql ="""	|select date,count(DISTINCT(user_id)) uv
				|from user
				|group by date
				|""".stripMargin

val dateUV = spark.sql(sql)
dateUV.coalesce(1).write.option("header","true").mode(SaveMode.Overwrite).csv("file:///home/data/date_uv")
```



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps3.jpg) |



 统计每天的页面访问量，主要代码如下：

```scala
val df = spark.read.orc(inpath)

val pv = df.groupBy("date").count().orderBy("date")

val datePV = pv.withColumnRenamed("count","pv")

datePV.show()
```

通过图4.2，图4.3可以看出，不管是PV还是UV趋势，均在12号的时候出现了一个尖峰，这正是著名的双十二大促节的用户集中消费导致的变化。

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps4.jpg) 

 

② 基于小时级别访问流量分析

统计基于小时的页面访问量，主要代码如下：

```scala
val df = spark.read.orc(inpath)
val pv = df.groupBy("hour").count().orderBy("hour")
val datePV = pv.withColumnRenamed("count", "pv").orderBy("hour")

```

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps5.jpg) 

 

统计基于小时的用户访问量，具体代码如下;

```python
df.createOrReplaceTempView("user")

val sql ="""|select hour,count(DISTINCT(user_id)) uv
			|from user group by hour""".stripMargin
val dateUV = spark.sql(sql).orderBy("hour")
```

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps6.jpg) 

​			          

  

图4.5 基于小时的用户访问量统计

通过图4.4，4.5可以看出，PV的高峰值出现在20点之后，可能的原因是淘宝的主力消费人群是工薪阶层，这部分群体在下班后开始使用淘宝浏览购物；UV的值比较恒定，上午10点之后便没有出现大的波动，一个可能的原因是用户早晨也会刷一下淘宝，比如看看物流状态，UV值在一天之内就不会再有大的变化波动了。 另外也可以看出，凌晨2点之后，PV/UV的趋势一致，均是一天中流量最小的时间段。

③不同用户行为流量分析

统计一个月内不同用户行为的次数及占比，主要代码如下：

```scala
val df = spark.read.option("header","true").csv(inpath)
val pv=df.groupBy("behavior_type").agg(sum("pv")"count").orderBy("behavior_type")
pv.show()
```



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps7.jpg) |

 

#### 4.2.2 转换率分析

转换率即所用用户的购买行为总数分别除以其它行为种类次数的总数。

①分析“浏览-收藏/加购-购买”链路的转化漏斗模型

```python
behavior_type = behaviorType_count["count"]
click_num,fav_num,add_num,pay_num=behavior_type[0],behavior_type[1],behavior_type[2], behavior_type[3]
print('点击到收藏转化率   {:.2f}%'.format(100 * fav_num / click_num))
print('点击到加购物车转化率{:.2f}%'.format(100 * add_num / click_num))
print('点击到购买转化率   {:.2f}%'.format(100 * pay_num / click_num))
print('加购物车到购买转化率{:.2f}%'.format(100 * pay_num / add_num))
print('收藏到购买转化率   {:.2f}%'.format(100 * pay_num / fav_num))
```

 

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps8.jpg) 



② 分析双十二“浏览-收藏/加购-购买”链路的转化漏斗模型

```scala
val df = spark.read.orc(inpath)
df.createOrReplaceTempView("user")
val sql = "select behavior_type,count(1) clicks from user where date='2014-12-12' group by behavior_type;"
val pv = spark.sql(sql)
pv.coalesce(1).write.option("header","true").mode(SaveMode.Overwrite).csv("file:///home/data/behavior_12_uv")}
behavior_12_uv = pd.read_csv("./behavior_12_uv.csv")
behavior_type = behavior_12_uv["clicks"]
click_num,fav_num,add_num,pay_num=behavior_type[1],behavior_type[2],behavior_type[0], behavior_type[3]
print('点击到收藏转化率   {:.2f}%'.format(100 * fav_num / click_num))
print('点击到加购物车转化率{:.2f}%'.format(100 * add_num / click_num))
print('点击到购买转化率   {:.2f}%'.format(100 * pay_num / click_num))
print('加购物车到购买转化率{:.2f}%'.format(100 * pay_num / add_num))
print('收藏到购买转化率   {:.2f}%'.format(100 * pay_num / fav_num))
```

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps9.jpg) 

  

#### 4.2.3 用户价值分析

在分析完整体变化趋势和转化率之后，商家更关注的是用户行为。商业上已经有不少成熟的模型可供参考，如用户价值RFM分析模型等，在此不做详细解释。本课题中还是从实际问题出发，站在用户购买行为的角度来探索用户价值。

① 用户购买频次分析

步骤1：统计每位用户在一个月内的购物次数

```scala
val df = spark.read.option("header","true").csv(inpath)
df.filer("behavior_type=4").groupBy("user_id").agg(count(1)).as("buys")
```

步骤2：使用pandas分析结果

```python
user_buy = pd.read_csv("./month_user_buys/month_user_buys.csv")
user_buy["buys"].describe()
```

 

|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps10.jpg) |

② ARPU分析

ARPU（Average Revenue Per User）表示每日的收益在所有活跃用户中的转化。详细的描述为，每日的所有收益与每日的活跃的用户数量有关，因此可以通过ARPU来计算，每日活跃用户对每日收益的转化效率。该数据集中没有对金额的体现。那我们可以对ARPU这个指标做一些修改，改为度量平台活跃用户每日平均消费次数。

计算公式为： ARPU = 每日消费总次数 / 每日活跃用户数

步骤1：计算每日消费总次数

```scala
val df = spark.read.option("header", "true").csv(inpath)
val dataUserArpu = df.groupBy("date").agg(sum("buys") as "buys").orderBy("date")
dataUserArpu.coalesce(1).write.option("header","true").mode(SaveMode.Overwrite).csv("file:///home/data/date_buys")}
```

步骤2：计算arpu

```python
date_uv = pd.read_csv("./date_uv/date_uv.csv")
date_buys = pd.read_csv("./date_buys/date_buys.csv")
date_uv["buys"] = date_buys["buys"]
arpu = date_uv.apply(lambda x : x["buys"] / x["uv"],axis=1).to_list()
plt.figure(figsize=(8,4))
plt.plot(date_uv["date"],arpu.to_list())
plt.xticks(rotation=90)
plt.title('ARPU')
plt.show()
```



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps11.jpg) |

 

③ ARPPU分析

ARPPU(average revenue per paying user)是指从每位付费用户中获得的收益，它反映的是整个平台的用户消费的均值。

它的计算方式为：ARPPU = 总收入/活跃用户付费数量。但是在该数据集中没有收益金额，因此我们可以对计算方式做一点转化，将总收入转化为总的购买行为次数。

定义如下：ARPPU = 当日总消费次数/当日活跃用户付费数量，可以看出和ARPU唯一的区别是分母，ARPU的分母是活跃用户数（包含4种行为类型），ARPPU的分母是活跃付费用户数，因此ARPPU的计算会更简单：

```python
user_buy = pd.read_csv("./date_user_buys/date_user_buys.csv")

def function1(data):
  l = len(data["user_id"].unique())
  c = data["buys"].sum()
  return c / l

arppu = user_buy.groupby("date").apply(function1)
arppu.plot()
plt.xticks(rotation=90)
plt.title('ARPPU')
```

 



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps12.jpg) |

④ 复购情况分析

一般来说，复购是指对产品的重复购买行为。但是这个定义在商业上是不精确的，假若一个用户在一天内多次在淘宝购买商品，不能说明这件用户对淘宝的依赖（有可能是某位用户只是第一次用，但是买的量大）。因此商业分析过程中，对于复购行为进行明确的定义。这里的复购是指：两天以上都在该平台产生了购买行为，需要指出一天多次的购买不算是复购。

步骤一：使用SparkSql获取复购人数以及复购天数。

```scala
val dateUB=spark.read.option("header","true").csv("file:///home/data/date_user_buys")

val result = dateUB.groupBy("user_id", "date").agg(count(col("buys")).as("count")) .filter(col("count") >= 2)
```

步骤二：使用Pandas进行数据分析。

```python
df = pd.read_csv("./repurchase.csv")
df.describ()
```



|      |                                                              |
| ---- | ------------------------------------------------------------ |
|      | ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps13.jpg) |

   

## 五、 数据挖掘

### 5.1 特征构造

特征是指从原始数据中提取出具有信息价值的属性和变量，在数据挖掘中一个好的特征能够提高数据挖掘的效率。本次课题主要从用户，商品，商品种类这三个维度构建特征。

#### 5.1.1 用户特征

① 用户行为最近发生时间

通过分组统计每位用户各种行为动作最接近的发生时间，字段如表5.1所示，代码实如下：

```scala
val userBehaviorFeatures = userBehavior_filtered.groupBy("user_id")

.agg(max(when(col("behavior_type")===1,time).as("u1_latest_click_time"),

max(when(col("behavior_type")===2, time).as("u1_latest_collect_time"),

max(when(col("behavior_type") === 3,time).as("u1_latest_add_cart_time"),

max(when(col("behavior_type")===4,time).as("u1_latest_buy_time"))
```

 

表5.1 用户最近行为发生时间

| 字段                    | 字段含义                   |
| ----------------------- | -------------------------- |
| user_id                 | 用户id                     |
| u1_latest_click_time    | 最近点击商品行为时间       |
| u1_latest_collect_time  | 最近收藏商品行为时间       |
| u1_latest_add_cart_time | 最近商品加入购物车行为时间 |
| u1_latest_buy_time      | 最近购买商品行为时间       |

 

② 用户行为次数

分组聚合统计用户点击量、收藏量、加购物车量、购买量，字段如表5.2所示。核心代码如下：

```scala
val userBehaviorCount = userBehaviorCount_1217.groupBy("user_id")

.agg(sum(when(col("behavior_type")===1,col("count"))).as("u2_click_count"),

​	sum(when(col("behavior_type")===2, col("`count`"))).as("u2_collect_count"),

 	sum(when(col("behavior_type")===3, col("`count`"))).as("u2_add_cart_count"),

​	sum(when(col("behavior_type") === 4, col("`count`"))).as("u2_buy_count") )
```

 

表5.2 用户各种行为动作次数统计

| 字段              | 字段含义                   |
| ----------------- | -------------------------- |
| user_id           | 用户id                     |
| u2_click_count    | 用户点击商品总次数         |
| u2_collect_count  | 用户收藏商品总次数         |
| u2_add_cart_count | 用户将商品加入购物车总次数 |
| u2_buy_count      | 用户购买商品总次数         |

 

③ 用户转化率

用户转化率即用户购买商品总次数分别除以用户点击、收藏、加购物车这三类行为总次数。字段如表5.3所示，核心代码如下：

```scala
val userPC = userBehaviorCount.select(

col("u2_click_count") / col("u2_collect_count") as "u3_click_count_pc", 

col("u2_click_count") / col("u2_add_cart_count") as "u3_collect_count_pc",

col("u2_click_count") / col("u2_buy_count") as "u3_add_cart_count_pc")
```

 

表5.3 用户转化率

| 字段                 | 字段含义                             |
| -------------------- | ------------------------------------ |
| user_id              | 用户id                               |
| u3_click_count_pc    | 用户购买商品数除以用户点击商品数     |
| u3_collect_count_pc  | 用户购买商品数除以用户收藏商品数     |
| u3_add_cart_count_pc | 用户购买商品数除以用户加购物车商品数 |

 

④ 用户行为方差与均值

计算用户点击、收藏、加购物车、购买量在28天里的均值方差，字段如表5.4所示。核心代码如下：

```scala
val userBehaviorStd = userBehaviorCount.groupBy("user_id")

.agg(avg(col("u2_click_count")).as("avg_click"),

  avg(col("u2_collect_count")).as("avg_collect"),

  avg( col("u2_add_cart_count")).as("avg_add_cart"),

  avg(col("u2_buy_count")).as("avg_buy"),

  stddev_samp(col("u2_click_count")).as("std_click"),

  stddev_samp(col("`u2_collect_count`")).as("std_collect"),

  stddev_samp(col("`u2_add_cart_count`")).as("std_add_cart"),

  stddev_samp( col("`u2_buy_count`")).as("std_buy") )
```

 

表5.4 用户行为方差与均值

| 字段名称     | 字段含义                   |
| ------------ | -------------------------- |
| user_id      | 用户id                     |
| avg_click,   | 用户平均点击商品数         |
| avg_collect  | 用户平均收藏商品数         |
| avg_add_cart | 用户平均将商品加入购物车数 |
| avg_buy      | 用户平均购买商品数         |
| std_click    | 用户点击商品数方差         |
| std_collect  | 用户收藏商品数方差         |
| std_add_cart | 用户将商品加入购物车数方差 |
| std_buy      | 用户购买商品数方差         |

 

#### 5.1.2 商品特征

① 商品行为总次数

计算商品被点击、收藏、加购物车总次数再除以商品被购买总次数就可以得到商品的购买转换率，字段如表5.5所示。核心代码如下：

```scala
val itemPC = """|SELECT item_id,

| (i1_buy_cnt / i1_click_cnt) as i2_buy_click_pc,

| (i1_buy_cnt / i1_collect_cnt) as i2_buy_collect_pc,

| (i1_buy_cnt / i1_addcart_cnt) as i2_buy_addcat_pc

|FROM item_behavior_cnt;""".stripMargin
```

 

表5.5 商品被购买转化率

| 字段名称          | 字段含义                   |
| ----------------- | -------------------------- |
| item_id           | 商品id                     |
| i2_buy_click_pc   | 商品被购买数除以点击数     |
| i2_buy_collect_pc | 商品被购买数除以收藏数     |
| i2_buy_addcat_pc  | 商品被购买数除以加购物车数 |

 

② 商品方差与均值

使用Spark SQL函数分组统计商品28天内被点击、被收藏、被加购物车、被购买次数的均值与方差，字段如表5.6所示。核心代码如下：

```scala
val sql3 = """|SELECT item_id,AVG(i1_click_cnt) AS avg_click_cnt,

​     | STDDEV_SAMP(i1_click_cnt) AS std_click_cnt,

​     | AVG(i1_collect_cnt) AS avg_collect_cnt,

​     | STDDEV_SAMP(i1_collect_cnt) AS std_collect_cnt,

​     | AVG(i1_addcart_cnt) AS avg_addcart_cnt,

|AVG(i1_buy_cnt) AS avg_buy_cnt,

​     | STDDEV_SAMP(i1_addcart_cnt) AS std_addcart_cnt,

| STDDEV_SAMP(i1_buy_cnt) AS std_buy_cnt

​     |FROM item_behavior_cntGROUP BY item_id;""".stripMargin
```

 

表5.6 商品均值与方差

| 字段名称        | 字段含义             |
| --------------- | -------------------- |
| item_id         | 商品id               |
| avg_click_cnt   | 商品被点击平均数     |
| std_click_cnt   | 商品被点击方差       |
| avg_collect_cn  | 商品被收藏平均数     |
| std_collect_cnt | 商品被收藏方差       |
| avg_addcart_cnt | 商品被加购物车平均数 |
| std_addcart_cnt | 商品被加购物车方差   |
| avg_buy_cnt     | 商品被购买平均数     |
| std_buy_cnt     | 商品被购买方差       |

 

#### 5.1.3 用户商品交叉特征

① 用户对商品的最近行为时间

以用户、商品、行为类型为维度，时间为度量值，通过Spark SQL窗口函数计算出用户对商品的各种行为发生的最近时间，字段如表5.7所示。核心代码如下：

```scala
val w1 = Window.partitionBy("user_id", "item_id").orderBy( desc("time"))

val latestTimeDF = userBehavior.withColumn("rank", row_number().over(w1))

​	.filter("rank = 1").groupBy("user_id", "item_id").agg(

​	max(when(col("behavior_type")===1,time)).as("ui1_latest_click_time"),

​	max(when(col("behavior_type")===2,time)).as("ui1_latest_favorite_time"),

​	max(when(col("behavior_type")===3,time)).as("ui1_latest_cart_time"),

​	max(when(col("behavior_type")===4,time)).as("ui1_latest_purchase_time") )
```

 

表5.7 用户对商品行为的最近时间

| 字段名称                 | 字段含义                     |
| ------------------------ | ---------------------------- |
| user_id                  | 用户id                       |
| item_id                  | 商品id                       |
| ui1_latest_click_time    | 用户对商品点击的最近时间     |
| ui1_latest_favorite_time | 用户对商品收藏的最近时间     |
| ui1_latest_cart_time     | 用户对商品加购物车的最近时间 |
| ui1_latest_purchase_time | 用户对商品购买的最近时间     |

 

② 用户对商品行为次数

以用户、商品为维度，各种行为类型总次数为度量值计算用户对商品的点击、收藏、加购物车、购买总次数，字段如5.8所示。核心代码如下：

```scala
val countDF = userBehavior.groupBy("user_id", "item_id")

​	.agg(sum(when(col("behavior_type") === 1, 1)).as("ui2_click_cnt"),

​	sum(when(col("behavior_type") === 2, 1)).as("ui2_favorite_cnt"),

​	sum(when(col("behavior_type") === 3, 1)).as("ui2_cart_cnt"),

​	sum(when(col("behavior_type") === 4, 1)).as("ui2_purchase_cnt") )
```

 

表5.8 用户对商品行为次数

| 字段名称         | 字段含义                     |
| ---------------- | ---------------------------- |
| user_id          | 用户id                       |
| item_id          | 商品id                       |
| ui2_click_cnt    | 用户对商品点击的最近时间     |
| ui2_favorite_cnt | 用户对商品收藏的最近时间     |
| ui2_cart_cnt     | 用户对商品加购物车的最近时间 |
| ui2_purchase_cnt | 用户对商品购买的最近时间     |

 

#### 5.1.4 商品种类特征

① 商品种类行为次数

以商品种类为维度，各种行为类型次数为度量值，计算该类商品被点击、收藏、加购物车、购买总次数，字段如表5.9所示。核心代码如下：

val categoryBehaviorCnt = df.groupBy("item_category").agg(

sum(when(col("behavior_type") === 1, 1)).as("c1_click_cnt"),

sum(when(col("behavior_type") === 2, 1)).as("c1_favorite_cnt"),

sum(when(col("behavior_type") === 3, 1)).as("c1_cart_cnt"),

sum(when(col("behavior_type") === 4, 1)).as("c1_purchase_cnt"))

 

表5.9 商品种类行为次数

| 字段名称        | 字段含义             |
| --------------- | -------------------- |
| item_category   | 商品种类id           |
| c1_click_cnt    | 该类商品被点击数     |
| c1_favorite_cnt | 该类商品被收藏数     |
| c1_cart_cnt     | 该类商品被加购物车数 |
| c1_purchase_cnt | 该类商品被购买数     |

 

② 商品种类转化率

以商品种类为维度，各种行为类型次数为度量值，计算该类商品被点击、收藏、加购物车、购买总次数，字段如表6.10所示。核心代码如下：

categoryBehaviorCnt.select(

​	col("item_category"),

​	col("click_cnt") / col("favorite_cnt") as "c2_click_favorite_pc",

​	col("click_cnt") / col("cart_cnt") as "c2_click_cart_pc",

​	col("click_cnt") / col("purchase_cnt") as "c2_click_purchase_pc")

 

表5.10 商品转化率

| 字段名称             | 字段含义                     |
| -------------------- | ---------------------------- |
| item_category        | 商品种类id                   |
| c2_click_favorite_pc | 该类商品购买数除以点击数     |
| c2_click_cart_pc     | 该类商品购买数除以加购物车数 |
| c2_click_purchase_pc | 该类商品购买数除以收藏数     |

 

#### 5.1.5商品种类-商品-用户交叉特征

① 用户对商品行为与用户商品种类行为占比

以用户、商品、商品种类、行为类型为维度，行为次数为度量值，统计用户对商品的点击、收藏、加购物车、购买次数除用户对该商品所属分类的点击、收藏、加购物车、购买次数，字段如表5.11所示。核心代码如下：

```scala
val category_cnt = category_item_cnt.groupBy("user_id","item_category").agg(
	sum(col("ui2_click_cnt")).as("category_click_cnt"),
	sum(col("ui2_favorite_cnt")).as("category_favorite_cnt"),
	sum(col("ui2_cart_cnt")).as("category_cart_cnt"),
	sum(col("ui2_purchase_cnt")).as("category_purchase_cnt"))

val userItemCategoryRatio = category_cnt.join(category_item_cnt,Seq("user_id","item_category"),"inner").select(
	col("user_id"),col("item_id"),col("item_category"),
	col("ui2_click_cnt") / col("category_click_cnt") as " click_ratio",
	col("ui2_favorite_cnt") / col("category_favorite_cnt") as "favorite_ratio",
	col("ui2_cart_cnt") / col("category_cart_cnt") as "cart_ratio",
	col("ui2_purchase_cnt") / col("category_purchase_cnt") as "purchase_ratio")
```

 

表5.11 用户对商品行为与用户商品种类行为占比

| 字段名称       | 字段含义                                                   |
| -------------- | ---------------------------------------------------------- |
| user_id        | 用户id                                                     |
| item_id        | 商品id                                                     |
| click_ratio    | 用户对商品的点击数除以用户对该商品所属分类的点击数         |
| favorite_ratio | 用户对商品的收藏数除以用户对该商品所属分类的收藏数         |
| cart_ratio     | 用户对商品的加购物车数除以用户对该商品所属分类的加购物车数 |
| purchase_ratio | 用户对商品的购买数除以用户对该商品所属购买数               |

 

② 用户对商品行为最近时间与用户商品种类行为最近时间差

以用户、商品为维度，时间为度量值，计算用户对商品最近点击、收藏、加购物车、购买时间减去该用户最近点击、收藏、加购物车、购买时间，字段如5.12所示。核心代码如下：

```scala
val f1 = userItemLatest.join(userBehaviorLatest,Seq("user_id"),"left").select(
col("user_id"),col("item_id"),
col("ui1_latest_click_time") - col("u1_latest_click_time") as "f1_diff_click_time",
col("ui1_latest_favorite_time")- col("u1_latest_collect_time") as "f1_diff_favorite_time",
col("ui1_latest_cart_time") - col("u1_latest_add_cart_time") as "f1_diif_cart_time",
col("ui1_latest_purchase_time") - col("u1_latest_buy_time") as "f1_diff_purchase_time")
```

③ 用户对商品行为最近时间与该用户最近购买时间的差

以用户、商品为维度，行为时间为度量值，计算用户对商品最近点击、收藏、加购物车、购买时间减去该用户购买时间，字段如表5.13所示。核心代码如下：

```scala
val f2 = userItemLatest.join(userBehaviorLatest, Seq("user_id"), "left").select(
	col("user_id"), col("item_id"),
	col("ui1_latest_click_time")-col("u1_latest_buy_time") as "f2_diff_click_time",
	col("ui1_latest_favorite_time")-col("u1_latest_buy_time") as "f2_diff_favorite_time",
	col("ui1_latest_cart_time") - col("u1_latest_buy_time") as "f2_diif_cart_time",
	col("ui1_latest_purchase_time") - col("u1_latest_buy_time") as "f2_diff_purchase_time")

 
```

表5.12用户对商品行为最近时间与用户商品种类行为最近时间差

| 字段名称              | 字段含义                                           |
| --------------------- | -------------------------------------------------- |
| user_id               | 用户id                                             |
| item_id               | 商品id                                             |
| f1_diff_click_time    | 用户对商品最近点击时间减去用户商品种类最近点击时间 |
| f1_diff_favorite_time | 用户对商品最近收藏时间减去用户商品种类最近收藏时间 |
| f1_diif_cart_time     | 用户对商品最近加购时间减去用户商品种类最近加购时间 |
| f1_diff_purchase_time | 用户对商品最近购买时间减去用户商品种类最近购买时间 |

 

表5.13 用户对商品行为最近时间与该用户最近购买时间的差

| 字段名称              | 字段含义                                         |
| --------------------- | ------------------------------------------------ |
| user_id               | 用户id                                           |
| item_id               | 商品id                                           |
| f2_diff_click_time    | 用户对商品最近点击时间减去该用户最近购买时间     |
| f2_diff_favorite_time | 用户对商品最近收藏时间减去该用户最近购买时间     |
| f2_diif_cart_time     | 用户对商品最近加购物车时间减去该用户最近购买时间 |
| f2_diff_purchase_time | 用户对商品最近购买时间减去该用户最近购买时间     |

 

### 5.2 特征选择

从特征中选择最相关或最重要的特征输入到机器学习算法中，可以减少不必要的特征输入，提高算法的训练效率和泛化能力[12]。特征选择的方法主要有基于相关性，基于模型，基于正则化。

基于相关性的特征选择：可以使用Spark的Correlation API计算每对特征之间的相关性，并使用VectorAssembler将数据集转换为特征向量。然后，可以使用ChiSqSelector来选择相关性高于某个阈值的特征，或者使用PearsonCorrelationSelector来选择与目标变量具有最高相关性的特征。

基于模型的特征选择：可以使用Spark的ML库中的各种机器学习算法进行特征选择。例如，可以使用RandomForestClassifier或GBTClassifier等算法训练模型，然后使用featureImportances属性查看特征的重要性，并选择最重要的特征。

基于正则化的特征选择：可以使用Spark的ML库中的正则化算法，例如Lasso或Ridge回归，来对特征进行选择。这些算法可以自动选择对目标变量最有影响的特征，并将其系数缩小到接近于零。

本次课题选择的是基于模型的特征选择，使用到的模型是RandomForest。

#### 5.2.1 特征预处理

①使用Saprk SQL将所有特征进行条件连接，连接方式选用left join避免连接无用数据。

②数据类型转换，将所有列转化为数值类型。

③缺失值处理：特征构建时会产生空值，空值全部替换成0。

核心代码如下所示：

```scala
val train = spark.read.orc("F://train")
val features = train.join(userBehavior_latset,Seq("user_id"),"left")
    .join(userBehavior_count, Seq("user_id"),"left")
    .join(userBehavior_percetconv, Seq("user_id"),"left")
    .join(userBehavior_std, Seq("user_id"),"left")
    .join(comdity_click_collect_addcart_buy_cnt, Seq("item_id"),"left")
    .join(comdity_click_collect_addcart_buy_pc, Seq("item_id"),"left")
    .join(comdity_click_collect_addcart_buy_std, Seq("item_id"),"left")
    .join(userItemBehavior_cnt,Seq("user_id", "item_id"),"left")
    .join(userItemBehavior_latest,Seq("user_id", "item_id"),"left")
    .join(categoryBehaviorCnt, Seq("item_category"),"left")
    .join(categoryBehaviorPC, Seq("item_category"),"left")
    .join(f8, Seq("item_id"),"left")
    .join(f1, Seq("user_id", "item_id"),"left")
    .join(f2, Seq("user_id", "item_id"),"left")
    .join(f3, Seq("user_id", "item_id"),"left")
    .join(f4, Seq("user_id", "item_id"),"left")
    .join(f5, Seq("user_id","item_id"),"left")

val dfDouble = features.select(features.columns.map(c => col(c).cast("double")): _*)
val df = dfDouble.na.fill(0,FeatureConfig.featureColmuns)
```



#### 5.2.2 基于模型选择特征

使用VectorAssembler将特征列合成向量，通过RandomForest模型进行训练，查看训练后每个特征的得分，选取等分大于等于0.1的特征。具体步骤按以下方式：

①本课题将用户对商品的购买行为作为正类样本，反之则为负类样本。由于正负样本分布差异过大，将对负类样本进行下采样达到与正类样本数据量相同，采样方法为随机采样。最后将数据集按7：3划分成训练集和验证集。

②将所有特征进行标准化后通过VectorAssembler合成为向量。

③使用Spark Mlib创建RandomForest模型并对数据进行训练。

④查看模型评分，并将训练时得分超过0.01的特征名称提取出来。

核心代码如下：

```scala
val sample0 = df.filter(col("label")===1)
val sample1 = df.filter(col("label")===0).sample(0.07)
val train = sample0.union(sample1)
val assembler = new VectorAssembler().setInputCols(features)).setOutputCol("features")
scaler=newStandardScaler().setInputCol("features").setOutputCol("features_scaler").setWithStd(true).setWithMean(false)
val pie = new Pipeline().setStages(Array(assembler,scaler))
val user_behavior= pie.fit(train).transform(train)
val Array(trainingData, testData) = user_behavior.randomSplit(Array(0.7, 0.3), seed = 6842L)
val rf = new RandomForestClassifier().setLabelCol("label").setFeaturesCol("features_scaler").setNumTrees(10)
val model = rf.fit(trainingData)
val predictions = model.transform(testData)
val featureScore = model.featureImportances.toArray.zipWithIndex
val selected = featureScore.filter(_._1>=0.01)map(_._2)
```

通过RandomForest模型选出了15条特征。分别是i1_click_cnt, i1_collect_cnt, i1_addcart_cnt，i1_buy_cnt，i2_buy_click_pc，i2_buy_collect_pc，i2_buy_addcat_pc，avg_click_cnt，std_click_cnt，avg_collect_cnt，std_collect_cnt，avg_addcart_cnt，std_addcart_cnt，avg_buy_cnt，std_buy_cnt。



### 5.3 模型实现

#### 5.3.1 模型评价标准

模型评价标准是衡量机器学习模型性能好坏的指标。不同的任务需要不同的评价指标，常见的分类模型评价指标有：

①Log Loss：Log Loss是一个用于二分类和多分类问题的评价指标，是模型预测结果与真实结果之间的交叉熵。Log Loss值越小，模型性能越好。

②AUC-ROC（Area Under the Receiver Operating Characteristic Curve）：AUC-ROC是用于二分类问题的评价指标，是指ROC曲线下的面积。ROC曲线横轴是False Positive Rate，纵轴是True Positive Rate。AUC-ROC越大，模型的性能越好。

③混淆矩阵（Confusion Matrix）：将实际分类与预测分类进行比较，得到的一个矩阵，可以用于计算准确率、精确率、召回率等指标。

本文在对模型进行评价时所采用的评价指标是混淆矩阵。

| ![img](file:///C:\Users\lenovo\AppData\Local\Temp\ksohtml4848\wps14.png)    预测类别真实类别 | 正类                           | 负类                           |
| ------------------------------------------------------------ | ------------------------------ | ------------------------------ |
| 正类                                                         | TP（将正类预测为正类的样本数） | FN（将负类预测为正类的样本数） |
| 负类                                                         | FP（将负类预测为正类的样本数） | TN（将负类预测为正类的样本数） |
| 总计                                                         | TP+FP                          | FN+TN                          |



####  5.3.2 模型实现与预测

① 逻辑回归模型

使用Spark Mlib中自带的LogisticRegression类创建逻辑回归模型，然后将经过选择后的特征合成向量传递给模型，设置模型的最大迭代次数MaxIter为10次；修改正则化系数RegParam为0.3防止模型过拟合；设置正则化范式比ElasticNetParamze为0.8，使用Elastic Net正则化降低模型的复杂度。核心代码如下：                                                                                                                                                                                             

```scala
val lr = new LogisticRegression().setElasticNetParam(0.9).setLabelCol("label").setMaxIter(10).setRegParam(0.3).setFeaturesCol("features")

val lrModel = lr.fit(trainingData)
```

使用训练后的模型预测用户购买行为，核心代码如下：

```scala
val predictions = lrModel.transform(testData)
```

经过模型的训练与验证后可以得到如表5.15所示的逻辑回归的混淆矩阵。

通过逻辑回归模型测试数据集的混淆矩阵可以计算出模型的精确率为0.6126342，召回率为0.6511899，F1-Score值为 0.6313239。

 

表5.15 逻辑回归测试数据集的混淆矩阵

| ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps18.png)     预测类别真实类别 | 正类   | 负类   |
| ------------------------------------------------------------ | ------ | ------ |
| 正类                                                         | 359563 | 192600 |
| 负类                                                         | 227350 | 305382 |
| 总计                                                         | 586913 | 49782  |

 

② 随机森林模型

使用Spark Mlib中自带的RandomForestClassifier类创建随机森林模型。由于随机森林模型会自主随机选择特征构建决策树，所以将所有的特征合称为向量传递给模型，然后修改随机森林中决策树的数量numTrees为10，经过训练后模型会选出最优的一颗决策树。核心代码如下：  

```scala
val rf = new RandomForestClassifier().setLabelCol("label").setFeaturesCol("features_scaler").setNumTrees(10).setFeatureSubsetStrategy("all")

val model = rf.fit(trainingData)
```

使用训练后的模型预测用户购买行为，核心代码如下：

```scala
val predictions = model.transform(testData)
```

通过表5.16可以计算出模型的精确率为0.667463，召回率为 0.666736，F1-Score值为 0.6670998。同时可以利用RandomForestClassifier的toDebugToString查看模型的结构，如图6.1所示。

| ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps19.png)    预测类别真实类别 | 正类   | 负类   |
| ------------------------------------------------------------ | ------ | ------ |
| 正类                                                         | 158122 | 79036  |
| 负类                                                         | 78778  | 149159 |
| 总计                                                         | 237158 | 227937 |

 

 

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps20.jpg)图5.1 随机森林模型结构

 

③ 梯度提升树模型

使用Spark Mlib中自带的GBTClassifier类创建随机梯度提升树模型。由于随机森林模型会自主随机选择特征构建决策树，所以将所有的特征合称为向量传递给模型，然后修改模型的迭代次数maxIter为10。核心代码如下

```scala
val gbdt = new GBTClassifier().setLabelCol("label").setFeaturesCol("features")

   .setMaxIter(10).setFeatureSubsetStrategy("auto")

val model = pipeline.fit(trainingData)
```

使用训练后的模型预测用户购买行为，核心代码如下：

```scala
val predictions = model.transform(testData)
```

通过梯度提升树模型测试数据集的混淆矩阵可以计算出模型的精确率为0.58687084，召回率为 0.8112228，F1-Score值为 0.6810459。同时，可以使用GBTClassifier的toDebugToString查看模型的结构,如图6.2所示。

 

表5.17 梯度提升树测试数据集的混淆矩阵

| ![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps21.png)    预测类别真实类别 | 正类   | 负类   |
| ------------------------------------------------------------ | ------ | ------ |
| 正类                                                         | 158122 | 79036  |
| 负类                                                         | 78778  | 149159 |
| 总计                                                         | 237158 | 227937 |

 

![img](https://github.com/one-day-days/spark-userbehavior/blob/main/src/main/resources/wps22.jpg)

 

| 模型名称   | 精确率     | 召回率    |
| ---------- | ---------- | --------- |
| 逻辑回归   | 0.6126342  | 0.6511899 |
| 随机森林   | 0.667463   | 0.666736  |
| 梯度提升树 | 0.58687084 | 0.8112228 |
